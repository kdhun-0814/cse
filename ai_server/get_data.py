import requests
from bs4 import BeautifulSoup
import csv
import time

TARGET_COUNT = 1000

# URLì„ ìª¼ê°­ë‹ˆë‹¤ (ê¸°ë³¸ ì£¼ì†Œ + íŒŒë¼ë¯¸í„°)
BASE_URL = "https://www.gnu.ac.kr/cse/na/ntt/selectNttList.do"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def collect_data_unique():
    print(f"ğŸ•·ï¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {TARGET_COUNT}ê°œ)")
    
    f = open('dataset.csv', 'w', encoding='utf-8-sig', newline='')
    wr = csv.writer(f)
    wr.writerow(['title', 'category']) 

    count = 0
    page = 1
    seen_titles = set()

    while count < TARGET_COUNT:
        # [í•µì‹¬ ìˆ˜ì •] URL ë’¤ì— ë¶™ì´ëŠ” ëŒ€ì‹ , params ë”•ì…”ë„ˆë¦¬ë¡œ ê¹”ë”í•˜ê²Œ ì „ë‹¬
        params = {
            'mi': 17093,
            'bbsId': 4753,
            'nttPageIndex': page  # í˜ì´ì§€ ë²ˆí˜¸ ìë™ ì ìš©
        }
        
        try:
            # paramsë¥¼ ë„£ì–´ì„œ ìš”ì²­
            response = requests.get(BASE_URL, headers=HEADERS, params=params)
            soup = BeautifulSoup(response.text, 'html.parser')
            rows = soup.select('tbody tr')
            
            if not rows: 
                print("   -> ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
                break
            
            new_in_page = 0
            
            for row in rows:
                # 1. 'ê³µì§€'ë¼ê³  ì íŒ ìƒë‹¨ ê³ ì •ê¸€ì€ ë¬´ì¡°ê±´ ê±´ë„ˆëœë‹ˆë‹¤.
                # (ì´ìœ : ëª¨ë“  í˜ì´ì§€ì— ì¤‘ë³µìœ¼ë¡œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— í—·ê°ˆë¦¼ ë°©ì§€)
                cols = row.select('td')
                if not cols: continue
                
                num_text = cols[0].get_text(strip=True)
                if "ê³µì§€" in num_text:
                    continue # ê³ ì • ê³µì§€ëŠ” ìˆ˜ì§‘ ì•ˆ í•¨ (ì¼ë°˜ ê¸€ë§Œ ìˆ˜ì§‘í•´ì„œ í•™ìŠµ)

                title_tag = row.select_one('a.nttInfoBtn')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                
                # ì¤‘ë³µ ì²´í¬
                if title in seen_titles:
                    continue
                
                # ë¶„ë¥˜ ë¡œì§ (ì„ì‹œ)
                category = "ì¼ë°˜"
                if "ì¥í•™" in title: category = "ì¥í•™"
                elif "ìˆ˜ê°•" in title or "í•™ì‚¬" in title or "ì„±ì " in title: category = "í•™ì‚¬"
                elif "ì±„ìš©" in title or "ì¸í„´" in title or "ëª¨ì§‘" in title: category = "ì·¨ì—…"
                elif "í–‰ì‚¬" in title or "ëŒ€íšŒ" in title or "íŠ¹ê°•" in title: category = "í–‰ì‚¬"

                wr.writerow([title, category])
                seen_titles.add(title)
                count += 1
                new_in_page += 1
                
                if count >= TARGET_COUNT: break
            
            # ë¡œê·¸ ì¶œë ¥: ì´ë²ˆ í˜ì´ì§€ì—ì„œ ì§„ì§œ ìƒˆë¡œìš´ ê¸€ì„ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
            if new_in_page > 0:
                print(f"ğŸ“„ {page}í˜ì´ì§€: {new_in_page}ê°œ ì €ì¥ ì™„ë£Œ (ëˆ„ì  {count}ê°œ)")
            else:
                print(f"ğŸ“„ {page}í˜ì´ì§€: ê±´ì§ˆ ê²Œ ì—†ìŒ (ë‹¤ ì¤‘ë³µì´ê±°ë‚˜ ê³µì§€)")

            page += 1
            time.sleep(0.3)
            
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            break
            
    f.close()
    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {count}ê°œ ì €ì¥ë¨.")

if __name__ == "__main__":
    collect_data_unique()