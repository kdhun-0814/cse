from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore
import time
import os
import json
import re
import csv
import requests # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ìš©
from datetime import datetime
from gemini_classifier import classify_notice_with_gemini

# ==========================================
# 1. Firebase ì ‘ì† ì„¤ì •
# ==========================================
if not firebase_admin._apps:
    firebase_key_json = os.environ.get('FIREBASE_KEY')
    if firebase_key_json:
        cred_dict = json.loads(firebase_key_json)
        cred = credentials.Certificate(cred_dict)
    else:
        if os.path.exists("serviceAccountKey.json"):
            cred = credentials.Certificate("serviceAccountKey.json")
        else:
            raise FileNotFoundError("Firebase í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")  
    firebase_admin.initialize_app(cred)

db = firestore.client()

# ==========================================
# 2. ì¡±ë³´ ë¡œë“œ
# ==========================================
manual_labels = {} 
try:
    with open('dataset.csv', 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        next(reader) 
        for row in reader:
            if len(row) >= 2:
                manual_labels[row[0].strip()] = row[1].strip()
    print(f"ğŸ“‚ ì¡±ë³´ ë¡œë“œ ì™„ë£Œ: {len(manual_labels)}ê°œ ë°ì´í„°")
except:
    print("âš ï¸ dataset.csv ì—†ìŒ. 100% Gemini ì˜ì¡´.")

# ==========================================
# ì„¤ì • ë° í•¨ìˆ˜
# ==========================================
START_URL = "https://www.gnu.ac.kr/cse/na/ntt/selectNttList.do?mi=17093&bbsId=4753"
BASE_HOST = "https://www.gnu.ac.kr" # ì´ë¯¸ì§€/íŒŒì¼ ê²½ë¡œ ê²°í•©ìš©
CUTOFF_DATE = "2023.01.01"

def check_deadline_urgency(title):
    try:
        match = re.search(r'~(\s*)(\d{1,2})[./](\d{1,2})', title)
        if match:
            month, day = int(match.group(2)), int(match.group(3))
            now = datetime.now()
            deadline = datetime(now.year, month, day)
            if deadline < now and (now.month - month) > 6:
                 deadline = datetime(now.year + 1, month, day)
            diff = (deadline - now).days
            if 0 <= diff <= 2: return True
    except: pass
    return False

# ==========================================
# [NEW] ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜
# ==========================================
def get_notice_detail(detail_url):
    """
    ìƒì„¸ í˜ì´ì§€ì— ì ‘ì†í•´ì„œ ë³¸ë¬¸, ì´ë¯¸ì§€, ì²¨ë¶€íŒŒì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        # ì…€ë ˆë‹ˆì›€ ëŒ€ì‹  requestsë¥¼ ì¨ì„œ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
        response = requests.get(detail_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. ë³¸ë¬¸ ë‚´ìš© (HTML íƒœê·¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ, ì¼ë°˜ì ì¸ êµ¬ì¡° íƒ€ê²ŸíŒ…)
        # ê²½ìƒëŒ€ í™ˆí˜ì´ì§€ êµ¬ì¡°ìƒ 'bbs_cntn' ë˜ëŠ” 'view_con' í´ë˜ìŠ¤ì— ë³¸ë¬¸ì´ ìˆìŒ
        content_div = soup.select_one('.bbs_cntn') or soup.select_one('.view_con')
        
        content_text = ""
        images = []
        
        if content_div:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ìœ ì§€)
            content_text = content_div.get_text('\n', strip=True)
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            img_tags = content_div.select('img')
            for img in img_tags:
                src = img.get('src')
                if src:
                    # ìƒëŒ€ ê²½ë¡œ(/resource/...)ë¥¼ ì ˆëŒ€ ê²½ë¡œ(https://...)ë¡œ ë³€í™˜
                    if src.startswith('/'):
                        src = BASE_HOST + src
                    images.append(src)

        # 2. ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
        files = []
        # íŒŒì¼ ì˜ì—­ ì°¾ê¸° (ë³´í†µ .file_area ë˜ëŠ” .bo_file)
        file_links = soup.select('.file_area a') or soup.select('.bo_file a')
        
        for file in file_links:
            f_name = file.get_text(strip=True)
            f_url = file.get('href')
            if f_url:
                if f_url.startswith('/'):
                    f_url = BASE_HOST + f_url
                files.append({
                    'name': f_name,
                    'url': f_url
                })

        return {
            'content': content_text,
            'images': images, # ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
            'files': files    # íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{name, url}]
        }

    except Exception as e:
        print(f"   âŒ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return {'content': 'ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.', 'images': [], 'files': []}


# ==========================================
# 3. í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜
# ==========================================
def crawl_gnu_cse(mode='all'):
    MAX_PAGE_LIMIT = 500 if mode == 'all' else 3
    print(f"ğŸ•·ï¸ ìµœì¢… ì‹œìŠ¤í…œ ê°€ë™ (ìƒì„¸ ë‚´ìš© í¬í•¨)")
    
    options = webdriver.ChromeOptions()
    options.add_argument('--headless') # ì°½ ì—†ì´ ì‹¤í–‰
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    driver.get(START_URL)
    time.sleep(3) 

    total_count = 0
    page = 1
    stop_crawling = False

    while not stop_crawling:
        if page > MAX_PAGE_LIMIT: break

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        rows = soup.select('tbody tr')
        
        # í˜ì´ì§€ ê²€ì¦
        check_title = "ì œëª©ëª»ì°¾ìŒ"
        for r in rows:
            if "ê³µì§€" not in r.select('td')[0].get_text():
                t = r.select_one('a.nttInfoBtn')
                if t: 
                    check_title = t.get_text(strip=True)[:10]
                    break
        print(f"\nğŸ“„ {page}í˜ì´ì§€ ìŠ¤ìº” ì¤‘ (ì¼ë°˜ê¸€: {check_title}...)")

        new_in_page = 0
        
        for row in rows:
            cols = row.select('td')
            if not cols: continue
            
            num_str = cols[0].get_text(strip=True)
            title_tag = row.select_one('a.nttInfoBtn')
            if not title_tag: continue
            
            title = title_tag.get_text(strip=True)
            link_id = title_tag['data-id']
            full_url = f"{BASE_HOST}/cse/na/ntt/selectNttInfo.do?mi=17093&bbsId=4753&nttSn={link_id}"
            
            date_str = ""
            for col in cols:
                text = col.get_text(strip=True)
                if re.match(r'^\d{4}\.\d{2}\.\d{2}$', text):
                    date_str = text
                    break
            
            # ë‚ ì§œ í•„í„°ë§
            if "ê³µì§€" not in num_str and date_str:
                if date_str < CUTOFF_DATE:
                    stop_crawling = True
                    break

            # --- DB ì²´í¬ ---
            doc_ref = db.collection('notices').document(link_id)
            doc = doc_ref.get()
            
            # [ì¤‘ìš”] ì´ë¯¸ ìˆê³ , ë‚´ìš©(content)ë„ ìˆìœ¼ë©´ ê±´ë„ˆëœ€ (ì‹œê°„ ì ˆì•½)
            if doc.exists and doc.to_dict().get('content'):
                continue 

            # --- ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ (ìƒˆ ê¸€ì´ê±°ë‚˜ ë‚´ìš©ì´ ì—†ì„ ë•Œë§Œ ì‹¤í–‰) ---
            print(f"   ğŸ” ìƒì„¸ ë‚´ìš© ê¸ëŠ” ì¤‘: {title[:15]}...")
            detail_data = get_notice_detail(full_url) # ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ í˜¸ì¶œ
            
            # --- ë¶„ë¥˜ ë¡œì§ ---
            IMPORTANT_KEYWORDS = ["ìˆ˜ê°•ì‹ ì²­", "ê¸°ìˆ™ì‚¬", "íœ´í•™", "ë³µí•™", "ì¡¸ì—…","êµ­ê°€ì¥í•™ê¸ˆ"]
            is_pinned_on_web = "ê³µì§€" in num_str
            has_important_keyword = any(keyword in title for keyword in IMPORTANT_KEYWORDS)
            is_important = is_pinned_on_web or has_important_keyword

            category = "í•™ì‚¬"
            if title in manual_labels:
                category = manual_labels[title]
            else:
                category = classify_notice_with_gemini(title)
                time.sleep(0.5) 

            is_deadline_imminent = check_deadline_urgency(title)
            is_urgent_display = is_important or is_deadline_imminent

            # --- ì €ì¥ ë°ì´í„° ì¤€ë¹„ ---
            save_data = {
                'title': title,
                'link': full_url,
                'date': date_str,
                'category': category,
                'is_important': is_important,
                'is_urgent': is_urgent_display,
                'author': "í•™ê³¼ì‚¬ë¬´ì‹¤",
                'is_manual': False,
                'crawled_at': firestore.SERVER_TIMESTAMP,
                
                # [ì¶”ê°€ë¨] ìƒì„¸ ë‚´ìš©ë“¤
                'content': detail_data['content'],
                'images': detail_data['images'], # ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
                'files': detail_data['files']    # íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            }

            doc_ref.set(save_data, merge=True) # merge=True: ê¸°ì¡´ í•„ë“œ ìœ ì§€í•˜ë©° ë®ì–´ì“°ê¸°
            new_in_page += 1
            
            # ìƒì„¸ í˜ì´ì§€ ì ‘ì† í…€ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(0.2)

        print(f"   -> {new_in_page}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        
        if stop_crawling: break

        # í˜ì´ì§€ ì´ë™
        page += 1
        try:
            driver.execute_script(f"goPaging({page});")
            time.sleep(2) 
        except Exception as e:
            print(f"âŒ ì´ë™ ì‹¤íŒ¨: {e}")
            break

    driver.quit()
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    crawl_gnu_cse(mode='all')