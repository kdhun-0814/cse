# ai_server/crawler.py
import requests
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore
import time
import os
import json
import re
import pickle # AI ëª¨ë¸ ë¡œë”©ìš©

# ==========================================
# 1. Firebase ì ‘ì† ì„¤ì •
# ==========================================
if not firebase_admin._apps:
    firebase_key_json = os.environ.get('FIREBASE_KEY')
    if firebase_key_json:
        # GitHub Actions í™˜ê²½
        cred_dict = json.loads(firebase_key_json)
        cred = credentials.Certificate(cred_dict)
    else:
        # ë¡œì»¬ í™˜ê²½
        if os.path.exists("serviceAccountKey.json"):
            cred = credentials.Certificate("serviceAccountKey.json")
        else:
            raise FileNotFoundError("Firebase í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")  
    firebase_admin.initialize_app(cred)

db = firestore.client()

# ==========================================
# 2. AI ëª¨ë¸ ë¡œë“œ (ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ë°©ì‹ ì‚¬ìš©)
# ==========================================
try:
    with open('model.pkl', 'rb') as f:
        ai_model = pickle.load(f)
    print("ğŸ§  AI ëª¨ë¸(model.pkl) ë¡œë“œ ì„±ê³µ!")
except:
    print("âš ï¸ AI ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    ai_model = None

BASE_HOST = "https://www.gnu.ac.kr"
BASE_URL = "https://www.gnu.ac.kr/cse/na/ntt/selectNttList.do?mi=17093&bbsId=4753"

# ==========================================
# 3. í¬ë¡¤ë§ í•¨ìˆ˜
# ==========================================
def crawl_gnu_cse(mode='update'):
    # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ì „ì²´ ìˆ˜ì§‘ ì‹œ 500, ì—…ë°ì´íŠ¸ ì‹œ 3)
    MAX_PAGE_LIMIT = 500 if mode == 'all' else 3
    
    print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘! ëª¨ë“œ: {mode} (ìµœëŒ€ {MAX_PAGE_LIMIT}í˜ì´ì§€)")
    
    page = 1
    stop_crawling = False 

    while not stop_crawling:
        if page > MAX_PAGE_LIMIT:
            print(f"   ğŸ›‘ ì•ˆì „ì¥ì¹˜ ë°œë™: {MAX_PAGE_LIMIT}í˜ì´ì§€ ë„ë‹¬. ì¢…ë£Œ.")
            break

        print(f"\nğŸ“„ {page}í˜ì´ì§€ ì½ëŠ” ì¤‘...", end=" ")
        target_url = f"{BASE_URL}&nttPageIndex={page}"
        
        try:
            response = requests.get(target_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            rows = soup.select('tbody tr')
            
            if not rows:
                print("-> ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            min_date_in_page = "9999.99.99" # í˜ì´ì§€ íë¦„ í™•ì¸ìš©

            for row in rows:
                # ì œëª© ë° ë§í¬ ì¶”ì¶œ
                title_tag = row.select_one('a.nttInfoBtn')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                link_id = title_tag['data-id'] # ê³ ìœ  ID
                full_url = f"{BASE_HOST}/cse/na/ntt/selectNttInfo.do?mi=17093&bbsId=4753&nttSn={link_id}"
                
                # í…Œì´ë¸” ì»¬ëŸ¼ ì¶”ì¶œ
                cols = row.select('td')
                num_str = cols[0].get_text(strip=True) # ë²ˆí˜¸ (ë˜ëŠ” 'ê³µì§€')
                
                # 1. ë‚ ì§œ ìë™ ì°¾ê¸° (ì •ê·œí‘œí˜„ì‹)
                date_str = ""
                for col in cols:
                    text = col.get_text(strip=True)
                    if re.match(r'^\d{4}\.\d{2}\.\d{2}$', text):
                        date_str = text
                        break
                
                if not date_str: continue

                # 2. ë‚ ì§œ íë¦„ ì²´í¬ (2022ë…„ ì´ì „ ì¤‘ë‹¨ ë¡œì§)
                # 'ê³µì§€'ê°€ ì•„ë‹Œ ì¼ë°˜ ê¸€ë§Œ ë‚ ì§œë¡œ ê³¼ê±°ì¸ì§€ íŒë‹¨
                if "ê³µì§€" not in num_str:
                    if date_str < min_date_in_page:
                        min_date_in_page = date_str
                    
                    if date_str < "2022.01.01":
                        print(f"\n   ğŸ›‘ {date_str} ë°œê²¬! 2022ë…„ ì´ì „ ë°ì´í„°ì´ë¯€ë¡œ í¬ë¡¤ë§ ì¢…ë£Œ.")
                        stop_crawling = True
                        break 
                
                # 3. ê³ ì •(Pin) ì—¬ë¶€ íŒë‹¨
                is_pinned = False
                if "ê³µì§€" in num_str:
                    is_pinned = True
                
                # 4. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (AI + ê·œì¹™ í•˜ì´ë¸Œë¦¬ë“œ)
                category = "ì¼ë°˜"
                
                if is_pinned:
                    category = "ê¸´ê¸‰" # ê³ ì • ê³µì§€ëŠ” ì¼ë‹¨ ê¸´ê¸‰ìœ¼ë¡œ
                elif ai_model:
                    # AIê°€ ì˜ˆì¸¡ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ [0]ìœ¼ë¡œ êº¼ëƒ„)
                    category = ai_model.predict([title])[0]
                else:
                    # AI ì—†ì„ ë•Œ ë°±ì—… ê·œì¹™
                    if "ì¥í•™" in title: category = "ì¥í•™"
                    elif "ìˆ˜ê°•" in title or "í•™ì‚¬" in title: category = "í•™ì‚¬"
                    elif "ì±„ìš©" in title or "ì¸í„´" in title: category = "ì·¨ì—…"
                    elif "í–‰ì‚¬" in title or "ëŒ€íšŒ" in title: category = "í–‰ì‚¬"
                
                # 5. DB ì €ì¥
                doc_ref = db.collection('notices').document(link_id)
                doc = doc_ref.get()
                
                if not doc.exists:
                    doc_ref.set({
                        'title': title,
                        'link': full_url,
                        'date': date_str,
                        'category': category,
                        'author': "í•™ê³¼ì‚¬ë¬´ì‹¤",
                        'is_pinned': is_pinned, # [í•µì‹¬] ê³ ì • ì—¬ë¶€ ì €ì¥
                        'is_manual': False,
                        'crawled_at': firestore.SERVER_TIMESTAMP
                    })
                    # print(".", end="", flush=True) # ì§„í–‰ë°”ì²˜ëŸ¼ ì  ì°ê¸°
                else:
                    pass

            if not stop_crawling:
                print(f"-> (~{min_date_in_page})")
            
            page += 1
            if not stop_crawling:
                time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€

        except Exception as e:
            print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
            break

if __name__ == "__main__":
    # ìµœì´ˆ ì‹¤í–‰ ì‹œ 'all', í‰ì†Œì—” 'update'
    crawl_gnu_cse(mode='update')