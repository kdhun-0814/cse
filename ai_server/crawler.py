from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore
import time
import os
import json
import re
import csv
from datetime import datetime
from gemini_classifier import classify_notice_with_gemini

# ==========================================
# 1. Firebase ì ‘ì† ì„¤ì •
# ==========================================
if not firebase_admin._apps:
    firebase_key_json = os.environ.get('FIREBASE_KEY')
    if firebase_key_json:
        cred_dict = json.loads(firebase_key_json)
        cred = credentials.Certificate(cred_dict)
    else:
        if os.path.exists("serviceAccountKey.json"):
            cred = credentials.Certificate("serviceAccountKey.json")
        else:
            raise FileNotFoundError("Firebase í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")  
    firebase_admin.initialize_app(cred)

db = firestore.client()

# ==========================================
# 2. ì¡±ë³´ ë¡œë“œ
# ==========================================
manual_labels = {} 
try:
    with open('dataset.csv', 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        next(reader) 
        for row in reader:
            if len(row) >= 2:
                manual_labels[row[0].strip()] = row[1].strip()
    print(f"ğŸ“‚ ì¡±ë³´ ë¡œë“œ ì™„ë£Œ: {len(manual_labels)}ê°œ ë°ì´í„°")
except:
    print("âš ï¸ dataset.csv ì—†ìŒ. 100% Gemini ì˜ì¡´.")

# ==========================================
# ì„¤ì •
# ==========================================
START_URL = "https://www.gnu.ac.kr/cse/na/ntt/selectNttList.do?mi=17093&bbsId=4753"
BASE_HOST = "https://www.gnu.ac.kr"
CUTOFF_DATE = "2023.01.01"

def check_deadline_urgency(title):
    try:
        match = re.search(r'~(\s*)(\d{1,2})[./](\d{1,2})', title)
        if match:
            month, day = int(match.group(2)), int(match.group(3))
            now = datetime.now()
            deadline = datetime(now.year, month, day)
            if deadline < now and (now.month - month) > 6:
                 deadline = datetime(now.year + 1, month, day)
            diff = (deadline - now).days
            if 0 <= diff <= 2: return True
    except: pass
    return False

# ==========================================
# 3. ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (Selenium ì‚¬ìš©)
# ==========================================
def scrape_detail_with_selenium(driver, url):
    try:
        # ìƒˆ íƒ­ ì—´ê¸° ë° ì´ë™
        driver.execute_script("window.open('');")
        driver.switch_to.window(driver.window_handles[1])
        driver.get(url)
        time.sleep(2) # ë¡œë”© ëŒ€ê¸°

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # [í•µì‹¬] ë³¸ë¬¸ ì°¾ê¸° ì „ëµ (ì‚¬ìš©ì ì œê³µ êµ¬ì¡° ê¸°ë°˜)
        # <tr class="cont"> <td colspan="2"> ... </td> </tr>
        content_html = ""
        images = []
        files = []

        # 1. ë³¸ë¬¸ (HTML êµ¬ì¡° ìœ ì§€)
        cont_row = soup.select_one('tr.cont')
        if cont_row:
             content_td = cont_row.select_one('td')
             if content_td:
                 # ì´ë¯¸ì§€ ê²½ë¡œ ì ˆëŒ€ì£¼ì†Œë¡œ ë³€í™˜
                 for img in content_td.select('img'):
                     src = img.get('src')
                     if src and src.startswith('/'):
                         img['src'] = BASE_HOST + src
                         images.append(img['src'])
                 
                 # style ì†ì„± ì¤‘ ë¶ˆí•„ìš”í•œ ê²ƒ ì œê±° or ìœ ì§€? 
                 # ëª¨ë°”ì¼ì—ì„œ ë³´ê¸°ì— ë„ˆë¬´ ë„“ì€ widthë‚˜ ê³ ì •ëœ heightì€ ì œê±°í•˜ëŠ”ê²Œ ì¢‹ìŒ
                 # ì¼ë‹¨ innerHTMLì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ë˜, ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
                 content_html = content_td.decode_contents()

        # 1-1. ë©”íƒ€ë°ì´í„° (ì‘ì„±ì, ì¡°íšŒìˆ˜, ë“±ë¡ì¼, ì œëª© ë“±) - í…Œì´ë¸” êµ¬ì¡° ë¶„ì„
        metadata = {'author': 'í•™ê³¼ì‚¬ë¬´ì‹¤', 'views': 0, 'date': ''} 
        try:
            # ë³´í†µ content_tr ìœ„ì— ë‹¤ë¥¸ trë“¤ì´ ìˆìŒ.
            # ë°©ë²•: "ì‘ì„±ì", "ì¡°íšŒìˆ˜" ë“±ì„ í¬í•¨í•˜ëŠ” th/td ì°¾ê¸°
            
            # ì‘ì„±ì
            author_tag = soup.find(string=re.compile("ì‘ì„±ì"))
            if author_tag:
                 # ë¶€ëª¨ë‚˜ í˜•ì œ ë…¸ë“œì—ì„œ ê°’ ì°¾ê¸°
                 # case: <th>ì‘ì„±ì</th><td>í™ê¸¸ë™</td>
                 author_td = author_tag.find_parent('th').find_next_sibling('td')
                 if author_td:
                     metadata['author'] = author_td.get_text(strip=True)
            
            # ì¡°íšŒìˆ˜
            views_tag = soup.find(string=re.compile("ì¡°íšŒìˆ˜|ì¡°íšŒ"))
            if views_tag:
                 views_td = views_tag.find_parent('th').find_next_sibling('td')
                 if views_td:
                     try:
                         metadata['views'] = int(re.sub(r'[^0-9]', '', views_td.get_text()))
                     except: pass
            
            # ì‘ì„±ì¼ (ë””í…Œì¼ í˜ì´ì§€ì— ìˆë‹¤ë©´ ê°€ì ¸ì˜¤ê¸°)
            date_tag = soup.find(string=re.compile("ë“±ë¡ì¼|ì‘ì„±ì¼"))
            if date_tag:
                 date_td = date_tag.find_parent('th').find_next_sibling('td')
                 if date_td:
                     metadata['date'] = date_td.get_text(strip=True)

        except Exception as e:
            print(f"   âš ï¸ ë©”íƒ€ íŒŒì‹± ì—ëŸ¬: {e}")

        # 1-2. ê²€ìƒ‰ìš© ìˆœìˆ˜ í…ìŠ¤íŠ¸ (Title + Content)
        content_text = ""
        if content_html:
            # HTML íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ
            text_soup = BeautifulSoup(content_html, 'html.parser')
            content_text = text_soup.get_text(separator=' ', strip=True)

        # ë§Œì•½ tr.contë¥¼ ëª» ì°¾ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹(ë°±ì—…) ì‹œë„
        if not content_html:
            content_div = soup.select_one('.bbs_cntn') or \
                          soup.select_one('.bdv_txt') or \
                          soup.select_one('.view_con')
            if content_div:
                for img in content_div.select('img'):
                     src = img.get('src')
                     if src and src.startswith('/'):
                         img['src'] = BASE_HOST + src
                         images.append(img['src'])
                content_html = content_div.decode_contents()

        # 2. ì²¨ë¶€íŒŒì¼ ì°¾ê¸° (ul.file)
        # <ul class="file"> <li> <a href="..."> ... </a> </li> </ul>
        file_ul = soup.select_one('ul.file')
        if file_ul:
            file_links = file_ul.select('a')
            for file in file_links:
                # "ë°”ë¡œë³´ê¸°" ë²„íŠ¼ ë“± ì œì™¸í•˜ê³  ë‹¤ìš´ë¡œë“œ ë§í¬ë§Œ
                href = file.get('href')
                if href and 'fileDown' in href and not href.startswith('javascript'):
                    f_name = file.get_text(strip=True)
                    # (ë‹¤ìš´ë¡œë“œ : 4íšŒ) ê°™ì€ í…ìŠ¤íŠ¸ ì œê±°í•˜ê³  íŒŒì¼ëª…ë§Œ ë‚¨ê¸°ê¸° ìœ„í•´ ì •ì œ ê°€ëŠ¥í•˜ë‚˜ ì¼ë‹¨ ê·¸ëŒ€ë¡œ ë‘ 
                    # í˜¹ì€ <strong> íƒœê·¸ ë‚´ìš© ì œê±°
                    for span in file.select('strong'):
                        span.extract()
                    f_name = file.get_text(strip=True)
                    
                    if href.startswith('/'): href = BASE_HOST + href
                    
                    if not any(f['url'] == href for f in files):
                        files.append({'name': f_name, 'url': href})
        
        # ê¸°ì¡´ ë°©ì‹ ë°±ì—… (ì²¨ë¶€íŒŒì¼)
        if not files:
            file_links = soup.select('.file_area a') or soup.select('.bo_file a')
            for file in file_links:
                f_name = file.get_text(strip=True)
                f_url = file.get('href')
                if f_url and not f_url.startswith('javascript'):
                    if f_url.startswith('/'): f_url = BASE_HOST + f_url
                    if not any(f['url'] == f_url for f in files):
                        files.append({'name': f_name, 'url': f_url})

        # íƒ­ ë‹«ê¸° ë° ë³µê·€
        driver.close()
        driver.switch_to.window(driver.window_handles[0])
        
        return {
            'content': content_html, 
            'text': content_text,
            'images': images, 
            'files': files,
            'metadata': metadata
        }

    except Exception as e:
        print(f"   âŒ ìƒì„¸ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        # ì—ëŸ¬ ë‚˜ë„ íƒ­ì€ ë‹«ì•„ì•¼ í•¨
        if len(driver.window_handles) > 1:
            driver.close()
            driver.switch_to.window(driver.window_handles[0])
        return {
            'content': '', 
            'text': '',
            'images': [],
            'files': [], 
            'metadata': {}
        }

# ==========================================
# 4. ë©”ì¸ í¬ë¡¤ëŸ¬
# ==========================================
def crawl_gnu_cse(mode='all', headless=True, page_limit=None):
    if page_limit:
        MAX_PAGE_LIMIT = page_limit
    else:
        MAX_PAGE_LIMIT = 500 if mode == 'all' else 3
    print(f"ğŸ•·ï¸ ìµœì¢… ì‹œìŠ¤í…œ ê°€ë™ (Selenium íƒ­ ì „í™˜ ë°©ì‹)")
    
    options = webdriver.ChromeOptions()
    if headless:
        options.add_argument('--headless')
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    driver.get(START_URL)
    time.sleep(2) 

    total_count = 0
    page = 1
    stop_crawling = False

    while not stop_crawling:
        if page > MAX_PAGE_LIMIT: break

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        rows = soup.select('tbody tr')
        
        # í˜ì´ì§€ ê²€ì¦
        check_title = "ì œëª©ëª»ì°¾ìŒ"
        for r in rows:
            if "ê³µì§€" not in r.select('td')[0].get_text():
                t = r.select_one('a.nttInfoBtn')
                if t: 
                    check_title = t.get_text(strip=True)[:10]
                    break
        print(f"\nğŸ“„ {page}í˜ì´ì§€ ìŠ¤ìº” ì¤‘ (ì¼ë°˜ê¸€: {check_title}...)")

        new_in_page = 0
        
        for row in rows:
            cols = row.select('td')
            if not cols: continue
            
            num_str = cols[0].get_text(strip=True)
            title_tag = row.select_one('a.nttInfoBtn')
            if not title_tag: continue
            
            title = title_tag.get_text(strip=True)
            link_id = title_tag['data-id']
            full_url = f"{BASE_HOST}/cse/na/ntt/selectNttInfo.do?mi=17093&bbsId=4753&nttSn={link_id}"
            
            # ë‚ ì§œ í™•ì¸
            date_str = ""
            for col in cols:
                text = col.get_text(strip=True)
                if re.match(r'^\d{4}\.\d{2}\.\d{2}$', text):
                    date_str = text
                    break
            
            # ë‚ ì§œ ì»·ì˜¤í”„
            if "ê³µì§€" not in num_str and date_str:
                if date_str < CUTOFF_DATE:
                    print(f"   ğŸ›‘ 2023ë…„ ì´ì „ ë°ì´í„° ë°œê²¬ ({date_str}). ì¢…ë£Œ.")
                    stop_crawling = True
                    break

            # --- DB ì¤‘ë³µ ì²´í¬ (ë‚´ìš© ìˆìœ¼ë©´ íŒ¨ìŠ¤) ---
            doc_ref = db.collection('notices').document(link_id)
            doc = doc_ref.get()
            
            # ë‚´ìš©(content)ê¹Œì§€ ì´ë¯¸ ê½‰ ì°¨ìˆìœ¼ë©´ ê±´ë„ˆëœ€
            if doc.exists and doc.to_dict().get('content'):
                continue 

            # --- [ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘] ---
            # Selenium ë¸Œë¼ìš°ì €ë¥¼ ê·¸ëŒ€ë¡œ ë„˜ê²¨ì¤˜ì„œ ì¿ í‚¤ ìœ ì§€!
            print(f"   ğŸ” ìƒì„¸ ìˆ˜ì§‘: {title[:10]}...", end="")
            detail_data = scrape_detail_with_selenium(driver, full_url)
            print(" ì™„ë£Œ")

            # --- ë¶„ë¥˜ ë¡œì§ (ì¤‘ìš”/ì¹´í…Œê³ ë¦¬/ê¸´ê¸‰) ---
            # ì¤‘ìš” ê³µì§€ í‚¤ì›Œë“œ í™•ì¥
            IMPORTANT_KEYWORDS = ["ìˆ˜ê°•ì‹ ì²­", "ê¸°ìˆ™ì‚¬", "íœ´í•™", "ë³µí•™", "ì¡¸ì—…", "êµ­ê°€ì¥í•™ê¸ˆ", "ë“±ë¡ê¸ˆ", "ì¥í•™ê¸ˆ"]
            is_pinned_on_web = "ê³µì§€" in num_str
            has_important_keyword = any(keyword in title for keyword in IMPORTANT_KEYWORDS)
            
            # ì¤‘ìš”: ì›¹ ê³ ì •(ê³µì§€ ë²ˆí˜¸)ì´ê±°ë‚˜ í‚¤ì›Œë“œ í¬í•¨ ì‹œ
            is_important = is_pinned_on_web or has_important_keyword

            category = "í•™ì‚¬"
            if title in manual_labels:
                category = manual_labels[title]
            else:
                category = classify_notice_with_gemini(title)
                time.sleep(0.5) 

            is_deadline_imminent = check_deadline_urgency(title)
            # ê¸´ê¸‰: ì¤‘ìš” ê³µì§€ì´ë©´ì„œ ë§ˆê° ì„ë°•ì¸ ê²½ìš° (ë˜ëŠ” ê´€ë¦¬ì ìˆ˜ë™ ì„¤ì •)
            # ì—¬ê¸°ì„œëŠ” 'ìë™' ê¸´ê¸‰ ë¡œì§ë§Œ ì„¤ì •
            is_urgent_display = is_important and is_deadline_imminent

            # --- ì €ì¥ ---
            final_author = detail_data['metadata'].get('author', "í•™ê³¼ì‚¬ë¬´ì‹¤")
            if final_author == "í•™ê³¼ì‚¬ë¬´ì‹¤" and "ì‘ì„±ì" in title: 
                 pass

            final_date = date_str 
            if detail_data['metadata'].get('date'):
                final_date = detail_data['metadata']['date']

            save_data = {
                'title': title,
                'link': full_url,
                'date': final_date,
                'category': category,
                'is_important': is_important,
                'is_urgent': is_urgent_display, # ì´ˆê¸°ê°’ (ê´€ë¦¬ìê°€ ë°”ê¿€ ìˆ˜ ìˆìŒ)
                
                'author': final_author,
                'views': detail_data['metadata'].get('views', 0),
                # views_todayëŠ” ì—¬ê¸°ì„œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ (0ìœ¼ë¡œ ë®ì–´ì“°ë©´ ì•ˆë¨)
                
                'is_manual': False,
                'crawled_at': firestore.SERVER_TIMESTAMP,
                
                'content': detail_data['content'],
                'content_text': detail_data['text'],
                'images': detail_data['images'], 
                'files': detail_data['files']
            }
            
            # views_today í•„ë“œê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì´ˆê¸°í™” (merge=Trueë¼ ê¸°ì¡´ ê°’ ìœ ì§€ë¨)
            # í•˜ì§€ë§Œ ë®ì–´ì“°ê¸° ìœ„í•´ set(merge=True) ì‚¬ìš©ì¤‘
            # setì„ ì“°ë©´ ì—†ëŠ” í•„ë“œëŠ” ë³´ì¡´ë˜ë‚˜? merge=Trueë©´ ë³´ì¡´ë¨.
            # ë‹¨, ìƒˆ ë¬¸ì„œì¼ ê²½ìš° views_todayê°€ ì—†ì„ ìˆ˜ ìˆìŒ.
            
            if not doc.exists:
                save_data['views_today'] = 0
            
            doc_ref.set(save_data, merge=True)
            new_in_page += 1

# ==========================================
# 5. ë°ì¼ë¦¬ ì¡°íšŒìˆ˜ ì´ˆê¸°í™” (ìì • ì‹¤í–‰ìš©)
# ==========================================
def reset_daily_views():
    print("ğŸŒ™ ìì • ì‘ì—…: ì¼ì¼ ì¡°íšŒìˆ˜(views_today) ì´ˆê¸°í™” ì‹œì‘...")
    batch = db.batch()
    count = 0
    
    # views_todayê°€ 0ë³´ë‹¤ í° ê²ƒë§Œ ê°€ì ¸ì™€ì„œ 0ìœ¼ë¡œ ë§Œë“¦
    docs = db.collection('notices').where('views_today', '>', 0).stream()
    
    for doc in docs:
        batch.update(doc.reference, {'views_today': 0})
        count += 1
        if count % 400 == 0: 
            batch.commit()
            batch = db.batch()
            
    if count > 0:
        batch.commit()
        
    print(f"âœ… ì´ {count}ê°œ ê³µì§€ì˜ ì¼ì¼ ì¡°íšŒìˆ˜ ì´ˆê¸°í™” ì™„ë£Œ.")
            
        print(f"   -> {new_in_page}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        
        if stop_crawling: break

        # í˜ì´ì§€ ì´ë™ (goPaging)
        page += 1
        try:
            driver.execute_script(f"goPaging({page});")
            time.sleep(2) 
        except Exception as e:
            print(f"âŒ ì´ë™ ì‹¤íŒ¨: {e}")
            break

    driver.quit()
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

if __name__ == "__main__":
    # [GitHub Actions / Cron ëª¨ë“œ]
    # ìŠ¤ì¼€ì¤„ëŸ¬ì— ì˜í•´ ì‹¤í–‰ë˜ë¯€ë¡œ ë£¨í”„ ì—†ì´ 1íšŒ ì‹¤í–‰ í›„ ì¢…ë£Œ
    # mode='recent' -> ì•ìª½ 3í˜ì´ì§€ë§Œ ë¹ ë¥´ê²Œ ìŠ¤ìº”
    print(f"â° ì •ê¸° í¬ë¡¤ë§ ì‹œì‘: {datetime.now()}")
    crawl_gnu_cse(mode='recent', headless=True)